{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6288fd5c-21dc-4c16-ac6e-ad484df301a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Linear Regression Consulting Project\n",
    "\n",
    "Congratulations! You've been contracted by Hyundai Heavy Industries to help them build a predictive model for some ships. [Hyundai Heavy Industries](http://www.hyundai.eu/en) is one of the world's largest ship manufacturing companies and builds cruise liners.\n",
    "\n",
    "You've been flown to their headquarters in Ulsan, South Korea to help them give accurate estimates of how many crew members a ship will require.\n",
    "\n",
    "They are currently building new ships for some customers and want you to create a model and use it to predict how many crew members the ships will need.\n",
    "\n",
    "Here is what the data looks like so far:\n",
    "\n",
    "    Description: Measurements of ship size, capacity, crew, and age for 158 cruise\n",
    "    ships.\n",
    "\n",
    "\n",
    "    Variables/Columns\n",
    "    Ship Name     1-20\n",
    "    Cruise Line   21-40\n",
    "    Age (as of 2013)   46-48\n",
    "    Tonnage (1000s of tons)   50-56\n",
    "    passengers (100s)   58-64\n",
    "    Length (100s of feet)  66-72\n",
    "    Cabins  (100s)   74-80\n",
    "    Passenger Density   82-88\n",
    "    Crew  (100s)   90-96\n",
    "    \n",
    "It is saved in a csv file for you called \"cruise_ship_info.csv\". Your job is to create a regression model that will help predict how many crew members will be needed for future ships. The client also mentioned that they have found that particular cruise lines will differ in acceptable crew counts, so it is most likely an important feature to include in your analysis! \n",
    "\n",
    "Once you've created the model and tested it for a quick check on how well you can expect it to perform, make sure you take a look at why it performs so well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf1da1f-ec59-45e5-9cc7-b93e1b5db44b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Methodology\n",
    "\n",
    "To solve the problem, a Random Forest Classifier was build and the more relevant features of the model were analized using the function 'featureImportances'. The features that best explain the accuracy of the model are the ones expected to be causing the problem that is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7290438e-a0dc-4087-a278-4307c81070df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Initialize session & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e26c68c-2a1c-4470-9f44-143f9d4c6580",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('tree').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a3c4ba-c409-44a8-8a1e-e00078f7d035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a053d8ca-9881-4f86-a2f7-80011f05efc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Read and analize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5021459e-c6fa-4298-b71b-17c703534239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv('/FileStore/tables/dog_food.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19081a40-d39f-4a1b-bc8c-6cd83d64d62c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+-------+\n|  A|  B|   C|  D|Spoiled|\n+---+---+----+---+-------+\n|  4|  2|12.0|  3|    1.0|\n|  5|  6|12.0|  7|    1.0|\n|  6|  2|13.0|  6|    1.0|\n|  4|  2|12.0|  1|    1.0|\n|  4|  2|12.0|  3|    1.0|\n| 10|  3|13.0|  9|    1.0|\n|  8|  5|14.0|  5|    1.0|\n|  5|  8|12.0|  8|    1.0|\n|  6|  5|12.0|  9|    1.0|\n|  3|  3|12.0|  1|    1.0|\n+---+---+----+---+-------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e965b7c-bfe2-4d3e-ab53-564f4a357a13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- A: integer (nullable = true)\n |-- B: integer (nullable = true)\n |-- C: double (nullable = true)\n |-- D: integer (nullable = true)\n |-- Spoiled: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae13c8f8-99a7-4e74-9c50-b700fcfca1a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Column C is composed of float numbers, to investigate if it's important or just a format issue, I am going to check its unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c439e933-a03b-464f-a589-7e9e8e34f19f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|   C|\n+----+\n| 8.0|\n| 7.0|\n|11.0|\n|14.0|\n|10.0|\n|13.0|\n| 6.0|\n| 5.0|\n| 9.0|\n|12.0|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "c_unique_values = data.select(\"C\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e1e95a-2af2-4dfc-9f53-c6d5134ddda5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It is just a format issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7866de-8fb2-4485-a1a4-067ddf7f03fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Feature engineering\n",
    "\n",
    "Although not strictly necessary, I'm going to convert column 'C' and label 'Spoiled' to integers for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38fe5ab5-0c28-4c9c-afbe-120f1c66c62e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa48a74b-3614-4a00-bea9-412962ed5755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The name of the 'Spoiled' column was changed to 'label'\n",
    "data_cast = data.select(col('A'), col('B'), col(\"C\").cast('int').alias(\"C\"), col('D'), col('Spoiled').cast('int').alias('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df11fb4e-fe1a-478c-891a-5edf7a81da9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- A: integer (nullable = true)\n |-- B: integer (nullable = true)\n |-- C: integer (nullable = true)\n |-- D: integer (nullable = true)\n |-- label: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data_cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4b3130-c3fd-42f2-a309-f5a01b8427ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+\n|  A|  B|  C|  D|label|\n+---+---+---+---+-----+\n|  4|  2| 12|  3|    1|\n|  5|  6| 12|  7|    1|\n|  6|  2| 13|  6|    1|\n|  4|  2| 12|  1|    1|\n|  4|  2| 12|  3|    1|\n+---+---+---+---+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_cast.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166fca45-a1f9-49f0-97d5-22ea087844c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Assembling a 'features' column\n",
    "\n",
    "Next, I'm going to assemble data from columns 'A', 'B', 'C' and 'D' in a column called 'features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c273c8-b13f-4a75-8e4c-505cc2ba5bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002eea88-1eaa-4377-8348-11c2d28e6005",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['A', 'B', 'C', 'D'], outputCol='features')\n",
    "data_assembled = assembler.transform(data_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd1d8f7-4f84-42a7-818d-61b6c73f23ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+------------------+\n|  A|  B|  C|  D|label|          features|\n+---+---+---+---+-----+------------------+\n|  4|  2| 12|  3|    1|[4.0,2.0,12.0,3.0]|\n|  5|  6| 12|  7|    1|[5.0,6.0,12.0,7.0]|\n|  6|  2| 13|  6|    1|[6.0,2.0,13.0,6.0]|\n|  4|  2| 12|  1|    1|[4.0,2.0,12.0,1.0]|\n|  4|  2| 12|  3|    1|[4.0,2.0,12.0,3.0]|\n+---+---+---+---+-----+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_assembled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01edc3eb-ae6a-48e0-a2d1-190acb88dcff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For clarity, a create a dataframe called 'final_data' that contains only the relevant columns for the model: 'features' and 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3cff4a-8bba-42ef-92c0-757a37200e02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_data = data_assembled.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bfe59c-a817-4183-95f2-b49b6ef3151d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n|          features|label|\n+------------------+-----+\n|[4.0,2.0,12.0,3.0]|    1|\n|[5.0,6.0,12.0,7.0]|    1|\n|[6.0,2.0,13.0,6.0]|    1|\n|[4.0,2.0,12.0,1.0]|    1|\n|[4.0,2.0,12.0,3.0]|    1|\n+------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15aedda5-9cb5-4805-8d9b-8b7a485d8eac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Building a random forest classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53252bd-1ce6-45e4-a6db-eba467730e33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First I'm going to split the data in training and test datasets. A train/test split is not necessary for this methodology, but I find it interesting to know the predictive power of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68d5839-3737-45b8-8d0a-eb5765630f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008cc7d9-62e0-4c84-9127-e37ad7d06200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Building of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ed3e2a-a1df-4828-8cfe-887a30442553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol='label', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f2398d-67e1-4337-8058-8e4ba59abf16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rfc_model = rfc.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a48043-826a-4a91-a7d3-7c4fab49a778",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Obtaining the predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc58a35-a852-4574-86d6-fe4f3dbf4d03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rfc_preds = rfc_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c4afb5b-0d38-4c90-8db8-ccd450c786ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluating the model\n",
    "\n",
    "The Area Under the Curve (AUC) and the average accuracy of the model were measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c4eb8b0-d842-46f5-9980-749f1266e0ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "binary_eval = BinaryClassificationEvaluator(labelCol='label')\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74178f3-045a-441e-9fed-5de2c9eb8345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "auc = binary_eval.evaluate(rfc_preds)\n",
    "acc = multi_eval.evaluate(rfc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e8db7c-d6ec-43a8-8cca-d9df3d29a402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9923371647509578\nAccuracy:  0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "print('AUC: ', auc)\n",
    "print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88029ec8-5665-42da-9fe7-26e928a16aad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The model predicts the spoiling of the food extremely well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7184c158-3c92-4fc8-8739-ab093fd27aee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Analysis of the importance of every feature in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd922444-1755-44ad-b8b8-4b8984aab944",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: SparseVector(4, {0: 0.0366, 1: 0.0224, 2: 0.923, 3: 0.018})"
     ]
    }
   ],
   "source": [
    "rfc_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5df7cde-f054-4ca4-9b7d-07ae76ab9698",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Component C (index 2) is clearly the one that explains the accuracy of the model and therefore it is the most probable candidate to be the agent causing the spoiling."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Random Forest Classification Consulting Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
