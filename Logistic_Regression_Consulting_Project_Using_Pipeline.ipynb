{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1029abb5-7389-485c-bfbf-0776e6db8d9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Logistic Regression Consulting Project Using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db963f0-33c9-41ce-9787-b28257736801",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "source": [
    "## Binary Customer Churn\n",
    "\n",
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.\n",
    "\n",
    "The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "    Name : Name of the latest contact at Company\n",
    "    Age: Customer Age\n",
    "    Total_Purchase: Total Ads Purchased\n",
    "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "    Years: Totaly Years as a customer\n",
    "    Num_sites: Number of websites that use the service.\n",
    "    Onboard_date: Date that the name of the latest contact was onboarded\n",
    "    Location: Client HQ Address\n",
    "    Company: Name of Client Company\n",
    "    \n",
    "Once you've created the model and evaluated it, test out the model on some new data (you can think of this almost like a hold-out set) that your client has provided, saved under new_customers.csv. The client wants to know which customers are most likely to churn given this data (they don't have the label yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a2e8da-1b81-4963-87fc-7848521edc6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imports & Build Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a28a391-0096-42fc-9abb-db87ae0701ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8b6001-ec94-45ec-b34c-c97dc6d13cda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('logreg5').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "890ff313-b508-4b9a-9b53-7d777a76266b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analysing database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a175e0-a980-47e6-9745-64f5c548bfc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_df = spark.read.csv('/FileStore/tables/customer_churn-2.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06afb804-7390-4448-8933-2cd65361186a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "churn_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d221ad56-13d1-45c1-846f-daef795a9d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Names: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Total_Purchase: double (nullable = true)\n |-- Account_Manager: integer (nullable = true)\n |-- Years: double (nullable = true)\n |-- Num_Sites: double (nullable = true)\n |-- Onboard_date: timestamp (nullable = true)\n |-- Location: string (nullable = true)\n |-- Company: string (nullable = true)\n |-- Churn: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "churn_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709bdf77-63db-4777-9e18-77c31235b3f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I'm going to consider or discard the different categories for the analysis:\n",
    "\n",
    " - Name : Is unique to the client so it will be out of the analysis. Besides, it's difficult to think that the name will have any influence in the clients decission to churn.\n",
    " - Age: Definitely could influence the clients decission to churn.\n",
    " - Total_Purchase: Also a relevant factor.\n",
    " - Account_Manager: The managers are assigned randomly, but the fact of having one may influence the clients' decission making, so I will include it.\n",
    " - Years: Clearly a factor of permanence or churning, it will be considered.\n",
    " - Num_sites: Again, it seems a relevant factor for the client.\n",
    " - Onboard_date: At first sight, it seems that the onboard date does not provide more info than 'Years', but I'm going to use the month when the client started the suscription to consider effects of seasonality. To do that, I need to extract the month from the date as a numerical value.\n",
    " - Location: The location of the client may be relevant since a bad reputation in a particular geographical region or regional competition could influence the clients' decission.\n",
    " - Company: Almost as unique as client name, so of no value for the analyisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be35bf43-e344-4506-9e6a-65da89374be7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19fa4c8-390c-4d80-a78a-07017d11a617",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Extracting state\n",
    "I'm going to build a function that extracts the state out of the company's address. For that, I'm going to use regular expressions to extract the state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62915f1-5064-40b6-b389-a178c45f8e36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ba8d46-dc7d-44db-b482-c23123e709cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_state_column(df):\n",
    "    return df.withColumn('State', regexp_extract('Location', '\\s[A-Z][A-Z]\\s', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8486db6c-c15a-4976-93e7-75606b543337",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dataframe containing a state code column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21604ef2-f2a2-4e63-9788-1fe3e50a75e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_state_df = add_state_column(churn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea4c216-3108-4426-b0ce-8ab7ba9ecaf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+-----+\n|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|State|\n+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+-----+\n|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|  AK |\n|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|  RI |\n|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|  DE |\n|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|  WY |\n|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|  MH |\n+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "churn_state_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8681a1a0-3808-4baa-b14b-8b1b43616c41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique states:  62\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique states: ', churn_clean_df.select('State').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90ec503-ddeb-43b8-a609-b82d41806fe7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Extract the onboard month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25dd8fbf-7c36-4bde-a0f6-710a0e9e0c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3ed82bd-f6f4-4338-a5b4-d0f8658c5ac1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dataframe containing a column with the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59817a5-3814-48e3-a757-517e3b321daa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_month_df = churn_state_df.withColumn('Onboard_month', month(churn_state_df['Onboard_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1450a3-2e50-4ffe-b61c-0ea31d841460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+-----+-------------+\n|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|State|Onboard_month|\n+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+-----+-------------+\n|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|  AK |            8|\n|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|  RI |            8|\n|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|  DE |            6|\n|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|  WY |            4|\n|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|  MH |            1|\n+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+-----+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "churn_month_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce573f59-2e00-4f48-91a4-0ae1203f8de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Analysing company name\n",
    "The number of companies is similar to the number of entries, so they are not useful for the analysis because they are almost every one unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322a9933-91b2-4aec-b162-1dbb6d279cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique companies:  873\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique companies: ', churn_month_df.select('Company').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad21952d-79c0-4df8-810d-979f711217b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Select columns\n",
    "As said, I will remove customer and company names because they are unique or close to unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224a0e67-67ee-44b7-9ca1-bc9bf37fcd34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: ['Names',\n 'Age',\n 'Total_Purchase',\n 'Account_Manager',\n 'Years',\n 'Num_Sites',\n 'Onboard_date',\n 'Location',\n 'Company',\n 'Churn',\n 'State',\n 'Onboard_month']"
     ]
    }
   ],
   "source": [
    "churn_month_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865d0405-2acd-4c64-a56e-60a1a5b61c50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_clean_df = churn_month_df.select(['Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites',\n",
    " 'Churn',\n",
    " 'State',\n",
    " 'Onboard_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce28381c-7dfe-4e03-9f5e-6b31d3c4964f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+-----+---------+-----+-----+-------------+\n| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|State|Onboard_month|\n+----+--------------+---------------+-----+---------+-----+-----+-------------+\n|42.0|       11066.8|              0| 7.22|      8.0|    1|  AK |            8|\n|41.0|      11916.22|              0|  6.5|     11.0|    1|  RI |            8|\n|38.0|      12884.75|              0| 6.67|     12.0|    1|  DE |            6|\n+----+--------------+---------------+-----+---------+-----+-----+-------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "churn_clean_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1377166-7045-44b0-92c8-59106819c3e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Indexing, One hot encoding, Assembling and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb96a525-e707-4ccd-92c7-a0fd305e5342",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this section I'm going to do the indexing (change categorical values to numerical), one-hot encoding (turn numerical values into vectors), the assembling (make a unique vector of al categories) and split the entries in training and test sets to analyse the model's performance. All this transformations will be stored to be executed in a Pipeline in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c671ad18-c5d2-466e-be19-298e97986461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d684fd22-cc71-4100-8aea-1fb681d36968",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state_indexer = StringIndexer(inputCol='State', outputCol='State_index')\n",
    "state_trained_indexer = state_indexer.fit(churn_clean_df)\n",
    "state_encoder = OneHotEncoder(inputCol='State_index', outputCol='State_vector')\n",
    "assembler = VectorAssembler(inputCols=['Age',\n",
    " 'Total_Purchase',\n",
    " 'Years',\n",
    " 'Account_Manager',\n",
    " 'Num_Sites',\n",
    " 'Onboard_month',\n",
    " 'State_vector'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62637a6a-0bec-4654-952b-ec0928fd67dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = churn_clean_df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37c879b-ae65-4cf1-a4a6-56e5fcb65e94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Building a Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847b6296-4ac2-4aa6-9794-ae72848da2a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df60ed12-c179-4fd5-9a63-efe75b922f63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features',labelCol='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198d006f-8ea3-4112-8a0b-ba62d41c30e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[state_indexer, state_encoder, assembler, log_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3267c42-56a9-49b6-be50-a92762082425",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train_data)\n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5a5cd9-bcc7-40cf-a402-28666a0f5f93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve:  0.7178883328427047\n"
     ]
    }
   ],
   "source": [
    "my_eval = BinaryClassificationEvaluator(labelCol='Churn')\n",
    "AUC = my_eval.evaluate(results)\n",
    "print('Area under the curve: ', AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2245d6b3-8999-4523-8ab1-36f084a02e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "multi_eval = MulticlassClassificationEvaluator(labelCol='Churn', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c33d8d6-3292-424f-bccc-375211e6a1e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy:  0.8821428571428571\n"
     ]
    }
   ],
   "source": [
    "acc = multi_eval.evaluate(results)\n",
    "print('Model accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f608365e-499c-4c6e-9d76-eadac00d66bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The model performs reasonably well for a dataset this short. The AUC is good and the accuracy is quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b409a24e-b7b8-4d1c-a233-e9b7a47458df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Testing against new data\n",
    "In this section I'm going to create a prediction with new unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0952efc3-6f55-4b1c-81a6-ed613f438e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Loading and visualizing new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ac9132-02ca-4b03-9dd4-e38c97243a44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = spark.read.csv('/FileStore/tables/new_customers-2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbb024c-211f-4a73-999f-4d928a8b0772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|\n+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|\n|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson|\n|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|\n|Megan Ferguson|32.0|        6487.5|              0|  9.4|     14.0|2016-10-28 05:32:13|922 Wright Branch...|   Sexton-Golden|\n|  Taylor Young|32.0|      13147.71|              1| 10.0|      8.0|2012-03-20 00:36:46|Unit 0789 Box 073...|        Wood LLC|\n| Jessica Drake|22.0|       8445.26|              1| 3.46|     14.0|2011-02-04 19:29:27|1148 Tina Straven...|   Parks-Robbins|\n+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0afb2b68-15e6-49fc-8614-0f68e1db4264",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I do the same data engineering than for the model-building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ef8cb9-4855-4352-92e8-4471d02dc31c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_clean_df = add_state_column(new_df).withColumn('Onboard_month', month(new_df['Onboard_date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d40628-e3d9-420e-87b9-59f949652220",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I create a new model using all the data available, not only the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3848cc-7bf8-4536-b71e-c7f253a88ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fit_model_all = pipeline.fit(churn_clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48dbfcac-8a37-4a38-955f-d9f5a6df47d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, I create a dataframe containing the predictions and select the relevant columns to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52f4f6b-79e5-4cbd-994d-786bb4cc0b6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_results = fit_model_all.transform(new_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1929fe-d732-4b7a-be86-65e97b9d4351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------+\n|         Names|         Company|prediction|\n+--------------+----------------+----------+\n| Andrew Mccall|        King Ltd|       0.0|\n|Michele Wright|   Cannon-Benson|       1.0|\n|  Jeremy Chang|Barron-Robertson|       1.0|\n|Megan Ferguson|   Sexton-Golden|       1.0|\n|  Taylor Young|        Wood LLC|       0.0|\n| Jessica Drake|   Parks-Robbins|       0.0|\n+--------------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "new_results.select('Names','Company','prediction').show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Logistic_Regression_Consulting_Project_Using_Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
