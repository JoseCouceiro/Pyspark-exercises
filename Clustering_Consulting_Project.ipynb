{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcd9d945-8a06-4cfb-9689-3c42ace3accd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Clustering Consulting Project \n",
    "\n",
    "A large technology firm needs your help, they've been hacked! Luckily their forensic engineers have grabbed valuable data about the hacks, including information like session time,locations, wpm typing speed, etc. The forensic engineer relates to you what she has been able to figure out so far, she has been able to grab meta data of each session that the hackers used to connect to their servers. These are the features of the data:\n",
    "\n",
    "* 'Session_Connection_Time': How long the session lasted in minutes\n",
    "* 'Bytes Transferred': Number of MB transferred during session\n",
    "* 'Kali_Trace_Used': Indicates if the hacker was using Kali Linux\n",
    "* 'Servers_Corrupted': Number of server corrupted during the attack\n",
    "* 'Pages_Corrupted': Number of pages illegally accessed\n",
    "* 'Location': Location attack came from (Probably useless because the hackers used VPNs)\n",
    "* 'WPM_Typing_Speed': Their estimated typing speed based on session logs.\n",
    "\n",
    "\n",
    "The technology firm has 3 potential hackers that perpetrated the attack. Their certain of the first two hackers but they aren't very sure if the third hacker was involved or not. They have requested your help! Can you help figure out whether or not the third suspect had anything to do with the attacks, or was it just two hackers? It's probably not possible to know for sure, but maybe what you've just learned about Clustering can help!\n",
    "\n",
    "**One last key fact, the forensic engineer knows that the hackers trade off attacks. Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have about 33 hacks. The engineer believes this is the key element to solving this, but doesn't know how to distinguish this unlabeled data into groups of hackers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bd5b603-7877-4f06-a2ef-d0846b865518",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Methodology\n",
    "To solve this problem, I'm going to use a two different clustering K-Means algorythm to classify the different attacks. One of them is going to try to group the data in two sets, while the other will try to do it in three sets. The one that fits the data better will tell us how many hackers were involved. Is important to note that this is possible because we know that all the hackers are expected to attack a similar number of times, otherwise the K-Means algorythm wouldn't be useful since it wouldn't discern well any minoritary group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e490c21b-1050-450a-9eaf-e7304c9e0d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Initialize session & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb23771-37cb-42b7-8a18-daeed42bb07f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8355436-4a31-4d7e-a464-b955180154a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('kmeans2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "407dc2a1-dd47-443b-89af-e17af2252852",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d24167-50a7-43e9-8216-5e07f16e3485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv('/FileStore/tables/hack_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327b96b9-f09f-4739-845d-365b7d048e06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|\n+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|\n|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|\n|                   31.0|           356.32|              1|             3.71|            8.0|             Tokelau|           70.58|\n|                    2.0|           228.08|              1|             2.48|            8.0|             Bolivia|            70.8|\n|                   20.0|            408.5|              0|             3.57|            8.0|                Iraq|           71.28|\n+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92734af1-e341-4215-a4b5-6ac59941f267",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The problem to solve is to determine if there are two or three different groups of data, that is, determine if the data adjust better to a clustering in two groups or three groups. With that objective in mind, I'm going to perform a Kmeans analysis on the sample. \n",
    "I'm going to use all the data except the location, given that in the description of the problem it is said that it is most probably irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b43a6c9-5081-4c34-b172-5d99d8c659ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Vector Assembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f1f92f1-f650-4657-b500-ce55f8cd3841",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "AS said, I'm going to make a vector containing the information of all the columns that will be used in the building of the model, that is all except location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889a9b68-fe60-4982-b44f-dea7cd2ee16a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88dcbce6-aec4-47af-bff2-9f10c2c2c59a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: ['Session_Connection_Time',\n 'Bytes Transferred',\n 'Kali_Trace_Used',\n 'Servers_Corrupted',\n 'Pages_Corrupted',\n 'Location',\n 'WPM_Typing_Speed']"
     ]
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9eae82-d92f-4b9c-b7c2-56b48470ea50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_columns = ['Session_Connection_Time',\n",
    " 'Bytes Transferred',\n",
    " 'Kali_Trace_Used',\n",
    " 'Servers_Corrupted',\n",
    " 'Pages_Corrupted',\n",
    " 'WPM_Typing_Speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec2d3d5-ce12-4003-b8ea-dc06f57a79d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=relevant_columns, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc64e32e-cbec-44fe-bbc9-60e206bfc163",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_data = assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e095f07-cd54-45f4-b602-f061b437fb95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cc1ed36-fc7b-4318-b950-f5307912ff98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Although the data is not differing in many orders of magnitude, I'm going to scale it to avoid biases because of the model giving more importance to certain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8d1e02-31a8-45c6-bd2e-692412f38784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668a5437-e89d-4834-93d3-8771eaed9073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6979021-96ca-4c8f-b39b-dd7d4309e039",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d696b8-920b-496c-af79-94872e152013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_data = scaler_model.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091b9ac4-f5da-4470-984e-76677ad91fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+--------------------+\n|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|            features|      scaledFeatures|\n+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+--------------------+\n|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|[8.0,391.09,1.0,2...|[0.56785108466505...|\n|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|[20.0,720.99,0.0,...|[1.41962771166263...|\n|                   31.0|           356.32|              1|             3.71|            8.0|             Tokelau|           70.58|[31.0,356.32,1.0,...|[2.20042295307707...|\n|                    2.0|           228.08|              1|             2.48|            8.0|             Bolivia|            70.8|[2.0,228.08,1.0,2...|[0.14196277116626...|\n|                   20.0|            408.5|              0|             3.57|            8.0|                Iraq|           71.28|[20.0,408.5,0.0,3...|[1.41962771166263...|\n+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c609339-b52a-4d3d-be82-39e940293317",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Kmeans Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16800def-458c-4cd5-a8a0-fc2f2cc238b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this section, I'm going to build the two models with 2 and 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbad7377-ef66-42cf-98e3-35a8c43caedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef3a91f-71d4-4f62-b409-2ffc95b5834e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(featuresCol='scaledFeatures', k=2, seed=77)\n",
    "kmeans_3 = KMeans(featuresCol='scaledFeatures', k=3, seed=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9803defa-9b6a-4170-8b00-25504a363234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_2 = kmeans_2.fit(final_data)\n",
    "model_3 = kmeans_3.fit(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884d332f-8000-48b9-8cb1-17c077fd40d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Evaluating the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9306a9d8-9065-4995-b1d5-789699f32139",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this section, I will analyse the accuracy of the models to determine which one adjust better to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef636048-74a5-4b47-9bd5-aa3b57ff262d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb9d190-2c33-4769-9963-0e361795fb1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds_2 = model_2.transform(final_data)\n",
    "preds_3 = model_3.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0bd19a-a3f5-4842-822c-3780be02d163",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette for 2 clusters:  0.6683623593283755\nSilhouette for 3 clusters:  0.3068084951287429\n"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "silhouette_2 = evaluator.evaluate(preds_2)\n",
    "silhouette_3 = evaluator.evaluate(preds_3)\n",
    "print(\"Silhouette for 2 clusters: \" , silhouette_2)\n",
    "print('Silhouette for 3 clusters: ', silhouette_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18cdffc4-60c7-4970-9d53-698bd09aa08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The model of 2 clusters gives a Silhouette score much closer to 1 than the 3 model, so we can conclude that it fits the data better.\n",
    "\n",
    "As a conclusion to the problem, we can claim that the number of hackers involved in the attack was 2.\n",
    "\n",
    "Besides, considering the initial assumption that if there were another perpetrator, they should have made the same number of attacks. I could see the following trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b4055c-3dd3-419e-99d0-d2c68e26b381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n|prediction|count|\n+----------+-----+\n|         1|  167|\n|         0|  167|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "preds_2.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89784ca-0baa-40e5-afe0-a41a28c4a078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n|prediction|count|\n+----------+-----+\n|         1|   84|\n|         2|   83|\n|         0|  167|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "preds_3.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8113a148-a5b2-4bda-b0b9-93ec5782b67a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The 2-clusters model splits the events evenly, but the 3-clusters model splits one of this groups in two, which confirms that there are only two evenly distributed groups in the sample."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clustering_Consulting_Project",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
